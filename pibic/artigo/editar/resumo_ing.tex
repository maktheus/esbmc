This work presents an investigation into the application of formal methods, specifically SMT-based (\textit{Satisfiability Modulo Theories}) model checking, to ensure safety and reliability in Generative Artificial Intelligence (GenAI) systems. Using the ESBMC (\textit{Efficient SMT-Based Context-Bounded Model Checker}), four case studies were explored: direct verification of Python models, memory safety in C++ inference engine kernels, neuro-symbolic refinement loops for LLM-generated code, and robustness of neural controllers under uncertainty (chaos engineering). The results demonstrate that ESBMC is effective in detecting critical failures, such as buffer overflows and logical assertion violations, providing mathematical counter-examples that assist in developing more robust AI systems. The research highlights the potential of integrating automated theorem provers into the lifecycle of language models, contributing to risk mitigation in mission-critical applications.

\textbf{Key-words}: Formal Verification; ESBMC; Generative Artificial Intelligence; Neural Networks; Software Safety; SMT.